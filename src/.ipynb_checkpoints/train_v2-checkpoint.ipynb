{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f86af3-7ebb-4bc5-a3d3-a3f90cc14953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as FF\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision.ops import FocalLoss as  FocalLossTV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "sys.path.append('libs/torchgeo')\n",
    "sys.path.append('libs/mmengine')\n",
    "sys.path.append('libs/mmcv')\n",
    "sys.path.append('libs/mmsegmentation')\n",
    "sys.path.append('libs/mmdetection')\n",
    "\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.logging import print_log\n",
    "from mmengine.optim.scheduler.lr_scheduler import PolyLR\n",
    "\n",
    "from mmseg.registry import RUNNERS\n",
    "from mmseg.models.losses import MSELoss, KLDistLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aebed31-7b11-4871-83d4-e6aae9fc9e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.2'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the whole train set --> 7470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7470 [00:00<?, ?it/s]/projects/Deep-Learning-For-Remote-Sensing/lcai_dataset.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  res['data_samples']['gt_sem_seg']  = torch.tensor(gt,dtype=torch.uint8)\n",
      " 14%|█▍        | 1080/7470 [00:51<05:06, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the whole test set --> 1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 270/1602 [00:10<00:51, 25.66it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('params.txt') as file:\n",
    "    params = file.read()\n",
    "params  = params.split('\\n')[0].split(',')\n",
    "dataset = params[0]\n",
    "arch    = params[1] #'swint'#'resnet50' # 'mnetv2' # mixvit\n",
    "decode_head = params[2]\n",
    "seed = int(params[3])\n",
    "lw0  = float(params[4])\n",
    "lw1  = float(params[5])\n",
    "lw2  = float(params[6])\n",
    "\n",
    "#dataset, arch, decode_head, seed , lw0, lw1, lw2 = 'lcai', 'resnet18', 'dlv3', 1, 0.0, 0.0, 0.0\n",
    "\n",
    "if dataset == 'lcai':\n",
    "    inp_channel  = 3\n",
    "    num_class    = 5\n",
    "    class_names  = 'bg,infra,water,green,road'\n",
    "    class_weight = [1.0,1.5,1.0,1.0,1.5] #[1.0,1.5,1.0,1.0,1.5]\n",
    "    cnames    = ['black', 'gray', 'forestgreen', 'cyan', 'blue']\n",
    "    img_size  = 128\n",
    "    epochs    = 15\n",
    "    pat       = 10\n",
    "    \n",
    "if dataset in ['naip','lsat']:\n",
    "    inp_channel  = 5\n",
    "    num_class    = 4\n",
    "    class_names  = \"bg,water,green,infra\"\n",
    "    if dataset == 'naip':\n",
    "        class_weight = [1.0,0.1,0.1,1.0]\n",
    "    if dataset == 'lsat':\n",
    "        class_weight = [1.0,1.0,1.0,1.0]\n",
    "    cnames = ['black','cyan','forestgreen','red']\n",
    "    img_size  = 128\n",
    "    epochs    = 10\n",
    "    pat       = 5\n",
    "    lr        = 5e-5\n",
    "\n",
    "if arch in ['unet','swint','mixvit']:\n",
    "    mean  = 0.0\n",
    "    optim = 'adam'\n",
    "    batch_size = 4\n",
    "    clip_grad  = None\n",
    "    \n",
    "if arch in ['resnet18', 'resnet34', 'resnet50', 'mnetv2']:\n",
    "    mean  = 0.5\n",
    "    optim = 'sgd'\n",
    "    batch_size = 4\n",
    "    clip_grad  = 1.0\n",
    "\n",
    "if optim == 'adam':\n",
    "   lr        = 5e-5\n",
    "   regularization = None#'L2'\n",
    "    \n",
    "if optim == 'sgd':\n",
    "   lr        = 0.01\n",
    "   regularization = 'L2'\n",
    "\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:21'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' # ':4096:8' :16:8\n",
    "\n",
    "if dataset == 'lcai':\n",
    "    from lcai_dataset import LandCoverDataset, CustomDataLoader\n",
    "    train_dataset = LandCoverDataset(split = 'train', img_size = img_size, aug_data = True, filter_data = True, mean = mean)\n",
    "    val_dataset   = LandCoverDataset(split = 'test', img_size = img_size, filter_data = False, mean = mean)\n",
    "    TrainDataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False, num_workers =0)\n",
    "    ValDataloader   = DataLoader(val_dataset, batch_size   = 3*batch_size, num_workers =0)\n",
    "\n",
    "if dataset in ['naip','lsat']:\n",
    "    from naip_dataset import NDVIDataset, GridGeoSampler, CustomDataLoader, Units\n",
    "    train_dataset  = NDVIDataset(split = 'train', name = dataset, img_size = img_size)\n",
    "    val_dataset    = NDVIDataset(split = 'val', name = dataset, img_size = img_size)\n",
    "\n",
    "    train_sampler   = GridGeoSampler(train_dataset, img_size, img_size, units=Units.PIXELS)\n",
    "    val_sampler     = GridGeoSampler(val_dataset, img_size, img_size, units=Units.PIXELS)\n",
    "    #train_sampler   = RandomGeoSampler(train_dataset, size=256) #, length=4)\n",
    "    #sampler        = RandomGeoSampler(ndvi_dataset, size=64) #, length=4)\n",
    "\n",
    "    TrainDataloader = CustomDataLoader(train_dataset, sampler=train_sampler, batch_size = batch_size)#, collate_fn=collate_fn, num_workers =0)\n",
    "    ValDataloader   = CustomDataLoader(val_dataset, sampler=val_sampler, batch_size = 3*batch_size, split = 'valid')#, collate_fn=collate_fn, num_workers =0)\n",
    "    \n",
    "if (lw1 > 0) and (lw2 > 0):\n",
    "    use_aux_head = decode_head\n",
    "else:\n",
    "    use_aux_head = ''\n",
    "\n",
    "config = 'libs/mmsegmentation/configs/deeplabv3plus/deeplabv3plus_r50-d8_4xb2-40k_cityscapes-769x769.py'\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "#cfg.optimizer = dict(type = 'Adam', lr = 0.0001)\n",
    "#dict\n",
    "cfg.aux1_lw = lw0\n",
    "cfg.aux2_lw = lw1\n",
    "cfg.aux3_lw = lw2\n",
    "cfg.T       = 1.0\n",
    "\n",
    "try:\n",
    "    lr_pfx = str(lr).split('.')[1]\n",
    "except:\n",
    "    lr_pfx = str(lr)\n",
    "    \n",
    "exp_path = f\"logs/v5/{optim}/{lr_pfx}/{dataset}_{arch}_{decode_head}_{num_class}cls_{lr}_{cfg.aux1_lw}_{cfg.aux2_lw}_{cfg.aux3_lw}_s{seed}\"\n",
    "\n",
    "if arch == 'unet':\n",
    "    channels = [64, 64, 64, 64, 64]\n",
    "    \n",
    "if arch == 'resnet18':\n",
    "    resnet_depth = 18\n",
    "    channels = [64, 128, 256, 512]\n",
    "    cfg.model.feat_channel = channels\n",
    "    init_cfg= None#{'type': 'Pretrained', 'checkpoint': 'torchvision://resnet18'}\n",
    "\n",
    "elif arch == 'resnet34':\n",
    "    resnet_depth = 34\n",
    "    channels = [64, 128, 256, 512]\n",
    "    cfg.model.feat_channel = channels\n",
    "    init_cfg= None#{'type': 'Pretrained', 'checkpoint': 'torchvision://resnet34'}\n",
    "    \n",
    "elif arch == 'resnet50':\n",
    "    resnet_depth = 50\n",
    "    channels = [256, 512, 1024, 2048]\n",
    "    cfg.model.feat_channel = channels\n",
    "    init_cfg = None #{'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
    "else:\n",
    "    resnet_depth = 34\n",
    "    channels = [64, 128, 256, 512]\n",
    "    cfg.model.feat_channel = channels\n",
    "    init_cfg = {'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
    "    \n",
    "if arch == 'mnetv2':\n",
    "    channels = [32, 96, 160, 320] # 32\n",
    "    cfg.model.feat_channel = channels\n",
    "    #init_cfg={'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
    "\n",
    "if arch == 'swint':\n",
    "    swint_ind = 1\n",
    "    channels = [[96, 192, 384, 768], [128, 256, 512, 1024]][swint_ind] \n",
    "    cfg.model.feat_channel = channels\n",
    "    #init_cfg={'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
    "    \n",
    "if arch == 'mixvit':\n",
    "    channels = [64, 128, 320, 512]\n",
    "    cfg.model.feat_channel = channels\n",
    "    #init_cfg={'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
    "\n",
    "cmap = colors.ListedColormap(cnames)\n",
    "bounds=np.arange(num_class + 1) - 0.5\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "decoder_loss = dict(type = 'CrossEntropyLoss', use_sigmoid = False, loss_weight = 1.0, class_weight = class_weight, ignore_index = None)\n",
    "#decoder_loss = dict(type = 'FocalLoss', use_sigmoid = True, loss_weight = 1.0, class_weight = class_weight)   \n",
    "itr_per_epoch = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "779e551b-c287-4731-92e2-9a1ab78e03a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(f'{exp_path}/src/libs/', exist_ok = True)\n",
    "os.system(f'cp -r libs/mmengine {exp_path}/src/libs/')\n",
    "os.system(f'cp -r libs/mmsegmentation {exp_path}/src/libs/')\n",
    "os.system(f'cp -r *py {exp_path}/src/')\n",
    "os.system(f'cp -r train_v2.ipynb {exp_path}/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f297d91-b084-484f-b075-215c219907a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 23\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainDataloader), len(ValDataloader))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d8cdce1-2f86-4779-8b42-2d86a0e8c7ab",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fig, axes = plt.subplots(nrows=10,ncols=4,figsize=(16,35))\n",
    "#axes      = axes.reshape(-1)\n",
    "\n",
    "for i,sample in enumerate(TrainDataloader):\n",
    "    axes[i,0].imshow(sample['inputs'][0][:3].permute(1,2,0)); axes[i,0].set_title('[a] RGB')\n",
    "    #axes[i,0].imshow(sample['data_samples'][0].gt_hres_img.data[:-2].permute(1,2,0)); axes[i,0].set_title('[a] 10m/Pix RGB')\n",
    "    axes[i,1].imshow(sample['data_samples'][0].gt_sem_seg.data.permute(1,2,0), cmap = cmap, norm = norm); axes[i,1].set_title('[b] Ground Truth Mask')\n",
    "    axes[i,2].imshow(sample['data_samples'][0].gt_hres_edge.data[0]); axes[i,2].set_title('[c] Class Edges')\n",
    "    #axes[i,4].imshow(sample['data_samples'][0].gt_hres_mask.data[:3].permute(1,2,0))\n",
    "    val, ind = sample['data_samples'][0].gt_hres_mask.data.max(axis=0); \n",
    "    axes[i,3].imshow(val); axes[i,3].set_title('[d] Saliency Maps')\n",
    "    #axes[i,5].imshow(cv2.resize(val.numpy(),(32,32)))\n",
    "    \n",
    "    if i == 9:\n",
    "        break\n",
    "for ax in axes.reshape(-1):\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61dbf394-0949-43ef-8b4a-c2d1aad1e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.savefig('inputs_cpk.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0aa54e-46ff-4a3f-a341-cf6c6aa3fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftCrossEntropyLoss(torch.nn.Module):\n",
    "   def __init__(self, loss_weight = 1.0, class_weight = [1.0], T = 1.0):\n",
    "      super().__init__()\n",
    "      self.lw = loss_weight\n",
    "      self.cw = torch.tensor(class_weight)[None,:,None,None]\n",
    "      self.T  = T\n",
    "\n",
    "   def forward(self, y_hat, y, **kwargs):\n",
    "      #y     = F.softmax(y/self.T, dim = 1)\n",
    "      y_hat = F.softmax(y_hat/self.T, dim = 1)\n",
    "      y     = self.cw.to(y.device)*y\n",
    "      loss  = -(y*y_hat.log()).sum(dim = 1)\n",
    "      return self.lw * loss.mean()\n",
    "       \n",
    "   @property\n",
    "   def loss_name(self):\n",
    "      return 'soft_ce_loss'\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "   def __init__(self, loss_weight = 1.0, class_weight = [1.0], T = 1.0):\n",
    "      super().__init__()\n",
    "      self.lw = loss_weight\n",
    "      self.cw = np.array(class_weight)\n",
    "      self.T  = T\n",
    "\n",
    "   def forward(self, y_hat, y, **kwargs):\n",
    "      #y     = F.softmax(y/self.T, dim = 1)\n",
    "      y_hat = F.softmax(y_hat/self.T, dim = 1)\n",
    "      #y     = self.cw[None,:,None,None]*y\n",
    "      loss  = -(y*y_hat.log()).sum(dim = 1)\n",
    "      return self.lw * loss.mean()\n",
    "       \n",
    "   @property\n",
    "   def loss_name(self):\n",
    "      return 'soft_ce_loss'\n",
    "\n",
    "class SoftFocalLoss(torch.nn.Module):\n",
    "   def __init__(self, loss_weight = 1.0, class_weight = [1.0], T = 1.0):\n",
    "      super().__init__()\n",
    "      self.lw = loss_weight\n",
    "      self.cw = np.array(class_weight)\n",
    "      self.T  = T\n",
    "\n",
    "   def forward(self, y_hat, y, alpha =  0.75, gamma = 2, **kwargs):\n",
    "      y_hat = F.sigmoid(y_hat/self.T)\n",
    "      ce_loss  = - ((y * y_hat.log()) + ((1 - y) * (1 - y_hat).log()))\n",
    "\n",
    "      p_t = y_hat * y + (1 - y_hat) * (1 - y)\n",
    "      loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "      if alpha >= 0:\n",
    "          alpha_t = alpha * y + (1 - alpha) * (1 - y)\n",
    "          loss    = alpha_t * loss\n",
    "      return self.lw * loss.mean()\n",
    "       \n",
    "   @property\n",
    "   def loss_name(self):\n",
    "      return 'soft_ce_loss'\n",
    "\n",
    "\n",
    "class SoftBinaryCrossEntropyLoss(torch.nn.Module):\n",
    "   def __init__(self, loss_weight = 1.0, T = 1.0):\n",
    "      super().__init__()\n",
    "      self.lw = loss_weight\n",
    "      self.T  = T\n",
    "\n",
    "   def forward(self, y_hat, y, **kwargs):\n",
    "      #y     = F.sigmoid(y/self.T)\n",
    "      y_hat = F.sigmoid(y_hat/self.T)\n",
    "      loss  = - ((y * y_hat.log()) + ((1 - y) * (1 - y_hat).log()))\n",
    "      return self.lw * loss.mean()\n",
    "\n",
    "   @property\n",
    "   def loss_name(self):\n",
    "      return 'soft_bce_loss'\n",
    "       \n",
    "class SoftKLDivLoss():\n",
    "   def __init__(self, weights = 1.0, T = 1.0):\n",
    "      super().__init__()\n",
    "      self.weights = weights\n",
    "\n",
    "   def forward(self, y_hat, y):\n",
    "      p = F.softmax(y_hat, 1)\n",
    "      y = self.weights*y\n",
    "      loss = (y*(F.log(y/p))).sum() / (w_labels).sum()\n",
    "      return loss\n",
    "       \n",
    "if arch == 'swint':      \n",
    "    swint =dict(\n",
    "            type='SwinTransformer',\n",
    "            pretrain_img_size=224,\n",
    "            embed_dims= [96, 128][swint_ind],\n",
    "            patch_size=4,\n",
    "            window_size=7,\n",
    "            mlp_ratio=4,\n",
    "            depths=[[2, 2, 6, 2], [2, 2, 18, 2]][swint_ind],\n",
    "            num_heads=[[3, 6, 12, 24], [4, 8, 16, 32]][swint_ind], \n",
    "            strides=(4, 2, 2, 2),\n",
    "            out_indices=(0, 1, 2, 3),\n",
    "            qkv_bias=True,\n",
    "            qk_scale=None,\n",
    "            patch_norm=True,\n",
    "            drop_rate=0.,\n",
    "            attn_drop_rate=0.,\n",
    "            drop_path_rate=0.3,\n",
    "            use_abs_pos_embed=False,\n",
    "            act_cfg=dict(type='GELU'),\n",
    "            norm_cfg = dict(type='LN', requires_grad=True))\n",
    "\n",
    "mixvit = dict(type = 'MixVisionTransformer',\n",
    "  in_channels = 3,\n",
    "  embed_dims  = 64,\n",
    "  num_stages  = 4,\n",
    "  num_layers  = [3, 4, 6, 3], # 2, 2, 2, 2\n",
    "  num_heads   = [1, 2, 5, 8],\n",
    "  patch_sizes = [7, 3, 3, 3],\n",
    "  sr_ratios   = [8, 4, 2, 1],\n",
    "  out_indices = (0, 1, 2, 3),\n",
    "  mlp_ratio   = 4,\n",
    "  qkv_bias    = True,\n",
    "  drop_rate   = 0.0,\n",
    "  attn_drop_rate = 0.0,\n",
    "  drop_path_rate = 0.1)\n",
    "  #init_cfg       = dict(type = 'Pretrained',\n",
    "  #checkpoint = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/segformer/mit_b1_20220624-02e5a6a1.pth'))\n",
    "\n",
    "resnet = dict(\n",
    "    type = 'ResNetV1c',\n",
    "    depth       = resnet_depth,\n",
    "    num_stages  = 4,\n",
    "    out_indices = (0, 1, 2, 3),\n",
    "    dilations   = (1, 1, 2, 4), # 1, 4, 8, 16 #1, 1, 2, 4\n",
    "    strides     = (1, 2, 2, 2), #1, 2, 2, 2\n",
    "    norm_cfg    = {'type': 'SyncBN', 'requires_grad': True},\n",
    "    norm_eval   = False,\n",
    "    style = 'pytorch',\n",
    "    contract_dilation = True,\n",
    "    init_cfg=init_cfg)\n",
    "\n",
    "mobilenet=dict(\n",
    "        type='MobileNetV2',\n",
    "        widen_factor=1.,\n",
    "        strides=(1, 1, 1, 2, 2, 1, 1), # (1, 2, 2, 1, 1, 1, 1)\n",
    "        dilations=(1, 1, 1, 2, 2, 4, 4),\n",
    "        out_indices=(2, 4, 5, 6), # 1, 2, 4, 6\n",
    "        norm_cfg=dict(type='SyncBN', requires_grad=True))\n",
    "\n",
    "unet = dict(\n",
    "        type='UNet',\n",
    "        in_channels=3,\n",
    "        base_channels=64,\n",
    "        num_stages=5,\n",
    "        strides=(1, 1, 1, 1, 1),\n",
    "        enc_num_convs=(2, 2, 2, 2, 2),\n",
    "        dec_num_convs=(2, 2, 2, 2),\n",
    "        downsamples=(True, True, True, True),\n",
    "        enc_dilations=(1, 1, 1, 1, 1),\n",
    "        dec_dilations=(1, 1, 1, 1),\n",
    "        with_cp=False,\n",
    "        conv_cfg=None,\n",
    "        norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
    "        act_cfg=dict(type='ReLU'),\n",
    "        upsample_cfg=dict(type='InterpConv'),\n",
    "        norm_eval=False)\n",
    "\n",
    "if decode_head == 'fcn':\n",
    "    fcn_head = dict(\n",
    "            type='FCNHead',\n",
    "            in_channels=64,\n",
    "            in_index=4,\n",
    "            channels=64,\n",
    "            num_convs=1,\n",
    "            concat_input=False,\n",
    "            dropout_ratio=0.1,\n",
    "            num_classes=num_class,\n",
    "            norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
    "            align_corners=False,\n",
    "            loss_decode=dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0))\n",
    "_nc = num_class\n",
    "if decode_head == 'dlv3':\n",
    "    if len(use_aux_head): dlv3_in_channels = 4*(max(channels[0]//2,32)) + _nc +1\n",
    "    else: dlv3_in_channels = channels[3]\n",
    "\n",
    "    deeplab_head = dict(\n",
    "         type = 'DepthwiseSeparableASPPHead',\n",
    "         in_channels = dlv3_in_channels,\n",
    "         in_index    = 1 if use_aux_head else 3,\n",
    "         #input_transform = 'resize_concat',\n",
    "         channels  = min(channels[1],256),\n",
    "         dilations = (1, 6, 12, 24), # 1, 12, 24, 36 # (1, 6, 12, 24)\n",
    "         c1_in_channels = channels[0],\n",
    "         c1_channels    = channels[0]//4, #48\n",
    "         dropout_ratio  = 0.1,\n",
    "         num_classes    = num_class,\n",
    "         activation     = None,\n",
    "         norm_cfg       = {'type': 'SyncBN', 'requires_grad': True},\n",
    "         align_corners  = True,\n",
    "         loss_decode    = dict(type = 'CrossEntropyLoss', use_sigmoid = False, loss_weight = 1.0, class_weight = class_weight))\n",
    "         #loss_decode   = dict(type = 'DiceLoss', use_sigmoid= False, loss_weight= 1.0))\n",
    "         #loss_decode    = dict(type = 'LovaszLoss', loss_type = 'multi_class', per_image = True))\n",
    "\n",
    "if decode_head == 'upn':\n",
    "    if len(use_aux_head): upn_in_channels  = [256 +1, 256 + num_class, 256, 256]\n",
    "    else: upn_in_channels  = channels\n",
    "        \n",
    "    upper_head = dict(type = 'UPerHead',\n",
    "      in_channels= upn_in_channels,\n",
    "      in_index= [0, 1, 2, 3],\n",
    "      pool_scales= (1, 2, 3, 6),\n",
    "      channels= 512,\n",
    "      dropout_ratio= 0.1,\n",
    "      num_classes= num_class,\n",
    "      norm_cfg= {'type': 'SyncBN', 'requires_grad': True},\n",
    "      align_corners = False,\n",
    "      loss_decode   = dict(type = 'CrossEntropyLoss', use_sigmoid= False, loss_weight= 1.0, class_weight = class_weight))\n",
    "\n",
    "if decode_head == 'sfm':\n",
    "    if len(use_aux_head): sfm_in_channels  = [256 +1, 256 + num_class, 256, 256]\n",
    "    else: sfm_in_channels  = channels\n",
    "\n",
    "    #16,24,32,64,96,160,320\n",
    "    segfm_head = dict(\n",
    "            type          = 'SegformerHead',\n",
    "            in_channels   = sfm_in_channels, #[32, 64, 160, 256] [96, 192, 384, 768], #\n",
    "            in_index      = [0, 1, 2, 3],\n",
    "            dilations     = [1, 1, 1, 1],\n",
    "            channels      = 256,\n",
    "            dropout_ratio = 0.1,\n",
    "            num_classes   = num_class,\n",
    "            activation    = None,\n",
    "            norm_cfg=dict(type = 'SyncBN', requires_grad=True),\n",
    "            align_corners      = False,\n",
    "            loss_decode=dict(type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0, class_weight = class_weight))\n",
    "\n",
    "if len(use_aux_head):\n",
    "    auxiliary_head_1 = dict(\n",
    "       type = 'FCNHead',\n",
    "       loss_key    = 'gt_hres_img',\n",
    "       in_channels = channels[0],\n",
    "       in_index    = 0,\n",
    "       channels    = channels[0]//2,\n",
    "       num_convs   = 1,\n",
    "       concat_input  = False,\n",
    "       dropout_ratio = 0.1,\n",
    "       num_classes   = 5,\n",
    "       activation    = None,\n",
    "       norm_cfg = {'type': 'SyncBN', 'requires_grad': True},\n",
    "       align_corners = True,\n",
    "       loss_decode   = dict(type = 'MSELoss', loss_weight = cfg.aux1_lw))\n",
    "    \n",
    "    auxiliary_head_2 = dict(\n",
    "       type = 'FCNHead',\n",
    "       loss_key    = 'gt_hres_edge',\n",
    "       in_channels = channels[0],\n",
    "       in_index    = 0,\n",
    "       channels    = channels[0]//2,\n",
    "       #input_transform = 'resize_concat',\n",
    "       num_convs     = 1,\n",
    "       concat_input  = False,\n",
    "       dropout_ratio = 0.1,\n",
    "       num_classes   = 1,\n",
    "       activation    = None,\n",
    "       norm_cfg = {'type': 'SyncBN', 'requires_grad': True},\n",
    "       align_corners = True,\n",
    "       loss_decode   = SoftBinaryCrossEntropyLoss(loss_weight = cfg.aux2_lw, T = cfg.T))\n",
    "    \n",
    "    auxiliary_head_3 = dict(\n",
    "       type = 'FCNHead',\n",
    "       loss_key    = 'gt_hres_mask',\n",
    "       in_channels =  channels[1],\n",
    "       in_index    =  1,\n",
    "       #input_transform = 'resize_concat',\n",
    "       channels    = channels[1]//2,\n",
    "       num_convs   = 1,\n",
    "       concat_input  = False,\n",
    "       dropout_ratio = 0.1,\n",
    "       num_classes   = _nc,\n",
    "       activation    = None,\n",
    "       norm_cfg      = {'type': 'SyncBN', 'requires_grad': True},\n",
    "       align_corners = True,\n",
    "       loss_decode   = SoftCrossEntropyLoss(loss_weight = cfg.aux3_lw, class_weight = class_weight, T = cfg.T))\n",
    "\n",
    "    cfg.model.auxiliary_head_1 = auxiliary_head_1\n",
    "    cfg.model.auxiliary_head_2 = auxiliary_head_2\n",
    "    cfg.model.auxiliary_head_3 = auxiliary_head_3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70b6a35-bce0-4480-a775-bcff740fb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cfg.model.auxiliary_head\n",
    "del cfg.model.pretrained\n",
    "cfg.model.use_aux_head = use_aux_head\n",
    "if arch == 'swint':\n",
    "   cfg.model.backbone = swint#resnet50\n",
    "if arch == 'mixvit':\n",
    "   cfg.model.backbone = mixvit\n",
    "if arch in ['resnet18','resnet34','resnet50']:\n",
    "   cfg.model.backbone = resnet\n",
    "if arch == 'mnetv2':\n",
    "    cfg.model.backbone = mobilenet\n",
    "if arch == 'unet':\n",
    "    cfg.model.backbone = unet\n",
    "    \n",
    "cfg.model.num_class = num_class\n",
    "\n",
    "if decode_head == 'dlv3':\n",
    "   cfg.model.decode_head   = deeplab_head\n",
    "if decode_head == 'upn':\n",
    "    cfg.model.decode_head  = upper_head\n",
    "if decode_head == 'sfm':\n",
    "    cfg.model.decode_head  = segfm_head\n",
    "if decode_head == 'fcn':\n",
    "    cfg.model.decode_head  = fcn_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370fa009-33ee-49de-8ca5-60c3bfd3b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#itr_per_epoch = 281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00a634a-a31f-4415-ac75-37970e237f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/20 15:18:40 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The prefix is not set in metric class IoUMetric.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/Deep-Learning-For-Remote-Sensing/libs/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:281: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mmseg.registry import MODELS, EVALUATOR\n",
    "from mmseg.utils import register_all_modules\n",
    "from mmseg.evaluation import IoUMetric\n",
    "\n",
    "#from mmdet.utils import register_all_modules as mmengine_register_all_modules\n",
    "#mmengine_register_all_modules()\n",
    "register_all_modules()\n",
    "#cfg.train_cfg = {'type' : 'EpochBasedTrainLoop' , 'max_epochs' :20, 'val_interval' :1 } #IterBasedTrainLoop\n",
    "cfg.train_cfg = {'type' : 'IterBasedTrainLoop' , 'max_iters' : epochs*itr_per_epoch, 'val_interval' : itr_per_epoch, 'output_dir' : exp_path}\n",
    "default_hooks = dict(checkpoint=dict(type='CheckpointHook', by_epoch = False, interval = -1, save_best = ['mIoU','mIoU1'], rule = ['greater','greater']))\n",
    "cfg.model.data_preprocessor['mean'] = [0.5] * inp_channel #[0.5, 0.5, 0.5]\n",
    "cfg.model.data_preprocessor['std']  = [1.0] * inp_channel\n",
    "cfg.model.data_preprocessor['size'] = (img_size,img_size)\n",
    "#cfg.model.test_cfg\n",
    "cfg.output_dir = exp_path\n",
    "\n",
    "model = MODELS.build(cfg.model)\n",
    "if arch == 'mnetv2':\n",
    "    model.backbone.conv1.conv = torch.nn.Conv2d(inp_channel, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "if arch in ['resnet18','resnet34','resnet50']:\n",
    "    model.backbone.stem[0] = torch.nn.Conv2d(inp_channel, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "if arch == 'swint':\n",
    "    model.backbone.patch_embed.projection = torch.nn.Conv2d(inp_channel, channels[0], kernel_size=(4, 4), stride=(4, 4))\n",
    "if arch == 'mixvit':\n",
    "    model.backbone.layers[0][0].projection = torch.nn.Conv2d(inp_channel, 64, kernel_size=(7, 7), stride=(7, 7))\n",
    "if arch == 'unet':\n",
    "    model.backbone.encoder[0][0].convs[0].conv = torch.nn.Conv2d(inp_channel, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    \n",
    "val_evaluator = IoUMetric(ignore_index = None, iou_metrics = ['mIoU','mFscore'], output_dir= exp_path, num_class =num_class, class_names = class_names, save_fig_int = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383788a-c76d-46eb-8395-b8986a7484b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if arch == 'resnet50':\n",
    "    mdl = torch.load('weights/deeplabv3plus_r50-d8_769x769_80k_cityscapes_20200606_210233-0e9dfdc4.pth')['state_dict']\n",
    "    ignore_keys = ['backbone.stem.0.','decode_head.conv_seg.weight', 'auxiliary_head.conv_seg.weight','decode_head.conv_seg.bias','auxiliary_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    for key in mdl.keys():\n",
    "        if inp_channel > 3 and key == 'backbone.stem.0':\n",
    "            continue\n",
    "        if 'auxiliary_head' in key or 'decode_head' in key:\n",
    "            continue\n",
    "        #if key not in ignore_keys:\n",
    "        new_mdl[key] = mdl[key]\n",
    "    model.load_state_dict(new_mdl,strict = False)\n",
    "\n",
    "if arch in ['resnet18', 'resnet34']:\n",
    "    mdl = torch.load('weights/deeplabv3plus_r18-d8_769x769_80k_cityscapes_20201226_083346-f326e06a.pth')['state_dict']\n",
    "    ignore_keys = ['backbone.stem.0.','decode_head.conv_seg.weight', 'auxiliary_head.conv_seg.weight','decode_head.conv_seg.bias','auxiliary_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    for key in mdl.keys():\n",
    "        if inp_channel > 3 and key == 'backbone.stem.0':\n",
    "            continue\n",
    "        if 'auxiliary_head' in key or 'decode_head' in key:\n",
    "            continue\n",
    "        #if key not in ignore_keys:\n",
    "        new_mdl[key] = mdl[key]\n",
    "    model.load_state_dict(new_mdl,strict = False)\n",
    "    \n",
    "if arch == 'unet':\n",
    "    mdl = torch.load('weights/fcn_unet_s5-d16_4x4_512x1024_160k_cityscapes_20211210_145204-6860854e.pth')['state_dict']\n",
    "    #backbone.encoder.0.0.\n",
    "    ignore_keys = ['decode_head.conv_seg.weight', 'decode_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    for key in mdl.keys():\n",
    "        if 'auxiliary_head' in key:\n",
    "            continue\n",
    "        if key in ignore_keys:\n",
    "            continue\n",
    "        new_mdl[key] = mdl[key]\n",
    "    model.load_state_dict(new_mdl,strict = False)\n",
    "\n",
    "if arch == 'swint':\n",
    "    paths =  ['weights/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210531_112542-e380ad3e.pth']\n",
    "    paths += ['weights/upernet_swin_base_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210526_192340-593b0e13.pth']\n",
    "    mdl = torch.load(paths[swint_ind])\n",
    "    ignore_keys = ['backbone.stem.0.','decode_head.conv_seg.weight', 'auxiliary_head.conv_seg.weight','decode_head.conv_seg.bias','auxiliary_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    ignore_keys = ['decode_head.conv_seg.weight', 'decode_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    for key in mdl.keys():\n",
    "        if 'auxiliary_head' in key:\n",
    "            continue\n",
    "        if key in ignore_keys:\n",
    "            continue\n",
    "        new_mdl[key] = mdl[key]\n",
    "    model.load_state_dict(new_mdl,strict = False)\n",
    "\n",
    "if arch == 'mixvit':\n",
    "    mdl = torch.load('weights/segformer_mit-b2_512x512_160k_ade20k_20210726_112103-cbd414ac.pth')['state_dict']\n",
    "    ignore_keys = ['backbone.stem.0.','decode_head.conv_seg.weight', 'auxiliary_head.conv_seg.weight','decode_head.conv_seg.bias','auxiliary_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    ignore_keys = ['decode_head.conv_seg.weight', 'decode_head.conv_seg.bias']\n",
    "    new_mdl = {}\n",
    "    for key in mdl.keys():\n",
    "        if 'auxiliary_head' in key:\n",
    "            continue\n",
    "        if key in ignore_keys:\n",
    "            continue\n",
    "        new_mdl[key] = mdl[key]\n",
    "    model.load_state_dict(new_mdl,strict = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d56a3-f6b6-4cfe-9f37-f4bdd718cdc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(use_aux_head):\n",
    "    cfg.model.auxiliary_head_2.loss_decode = cfg.model.auxiliary_head_2.loss_decode.loss_name\n",
    "    cfg.model.auxiliary_head_3.loss_decode = cfg.model.auxiliary_head_3.loss_decode.loss_name\n",
    "with open(f'{exp_path}/config.json', 'w') as file:\n",
    "    json.dump(cfg.to_dict(), file)\n",
    "model.to('cuda')\n",
    "if len(use_aux_head):\n",
    "    for head in model.auxiliary_heads:\n",
    "        head.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768c227-dc05-41c5-bf15-443100cadcdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import *\n",
    "if optim == 'sgd':\n",
    "    optimizer = SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay = 0.0005)\n",
    "    scheduler = None#PolyLR(optimizer, eta_min = 0.0001, power = 0.9, begin = 0, end = 40000, by_epoch = False)\n",
    "    #scheduler = PolynomialLR(optimizer, total_iters = 40000, power=0.9, last_epoch=-1)\n",
    "if optim == 'adam':\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = None\n",
    "    \n",
    "train(model, \n",
    "      TrainDataloader, \n",
    "      ValDataloader, \n",
    "      epochs, \n",
    "      optimizer, \n",
    "      scheduler,\n",
    "      evaluator = val_evaluator,\n",
    "      clip_grad = clip_grad,\n",
    "      regularization = regularization,#\"L2\", \n",
    "      reg_lambda = 1e-6, \n",
    "      patience = pat, \n",
    "      verbose = True, \n",
    "      device = 'cuda', \n",
    "      output_dir = exp_path)\n",
    "torch.save(model.state_dict(), f'{exp_path}/final_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79e6f53-9321-4cee-a922-9d358a0bf5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the whole test set --> 1602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (data_preprocessor): SegDataPreProcessor()\n",
       "  (backbone): ResNetV1c(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): ResLayer(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "    )\n",
       "    (layer2): ResLayer(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "    )\n",
       "    (layer3): ResLayer(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "    )\n",
       "    (layer4): ResLayer(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm2'}}\n",
       "    )\n",
       "  )\n",
       "  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]\n",
       "  (decode_head): DepthwiseSeparableASPPHead(\n",
       "    input_transform=None, ignore_index=255, align_corners=True\n",
       "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (conv_seg): Conv2d(128, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (image_pool): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=1)\n",
       "      (1): ConvModule(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activate): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (aspp_modules): DepthwiseSeparableASPPModule(\n",
       "      (0): ConvModule(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activate): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): DepthwiseSeparableConvModule(\n",
       "        (depthwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=512, bias=False)\n",
       "          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (pointwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DepthwiseSeparableConvModule(\n",
       "        (depthwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=512, bias=False)\n",
       "          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (pointwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): DepthwiseSeparableConvModule(\n",
       "        (depthwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=512, bias=False)\n",
       "          (bn): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (pointwise_conv): ConvModule(\n",
       "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bottleneck): ConvModule(\n",
       "      (conv): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (c1_bottleneck): ConvModule(\n",
       "      (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "    (sep_bottleneck): Sequential(\n",
       "      (0): DepthwiseSeparableConvModule(\n",
       "        (depthwise_conv): ConvModule(\n",
       "          (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn): SyncBatchNorm(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (pointwise_conv): ConvModule(\n",
       "          (conv): Conv2d(144, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DepthwiseSeparableConvModule(\n",
       "        (depthwise_conv): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "        (pointwise_conv): ConvModule(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "val_evaluator = IoUMetric(ignore_index = None, iou_metrics = ['mIoU','mFscore'], output_dir= 'test', num_class =num_class, class_names = class_names)\n",
    "if dataset == 'lcai':\n",
    "   val_dataset   = LandCoverDataset(split = 'test', img_size = img_size, filter_data = False, mean = mean)\n",
    "   ValDataloader = DataLoader(val_dataset, batch_size  = 3*batch_size)\n",
    "model.train()\n",
    "\n",
    "#exp_path = 'logs/005/lcai_v3_resnet50_dlv3_5cls_0.005_0.0_0.0_0.0_s1'\n",
    "path = f'{exp_path}/checkpoint.pt'\n",
    "mdl = torch.load(path)#['state_dict']\n",
    "model.load_state_dict(mdl)\n",
    "model.to('cuda')\n",
    "if len(use_aux_head):\n",
    "    for head in model.auxiliary_heads:\n",
    "        head.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3303f34-8972-432f-8072-d8bcac2cbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm(x):\n",
    "    x = torch.nn.functional.softmax(x, dim=1)\n",
    "    return x\n",
    "    \n",
    "def sg(x):\n",
    "    x = torch.nn.functional.sigmoid(x)\n",
    "    return x\n",
    "    \n",
    "def img_norm(x):\n",
    "    #x = x - x.min(axis=-1)[0].min(axis=-1)[0][...,None,None]\n",
    "    #x = x / x.max(axis=-1)[0].max(axis=-1)[0][...,None,None]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35b08e-ec53-4cc7-bd18-5ffbd050193b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "os.makedirs(f'{exp_path}/figs', exist_ok = True)\n",
    "os.makedirs(f'{exp_path}/res', exist_ok = True)\n",
    "#ValDataloader   = CustomDataLoader(val_dataset, batch_size   = batch_size, split = 'valid')#,collate_fn=collate_fn, num_workers =0)\n",
    "titles = ['Image', 'Ground Truth', 'Prediction', 'Edges', 'Saliency']\n",
    "\n",
    "def dec_io(data_batch, outputs, aux_feat, atn):\n",
    "    imgs  = []\n",
    "    masks = []\n",
    "    preds = []\n",
    "    aux_feats_0 =[]\n",
    "    aux_feats_1 = []\n",
    "    atns = []\n",
    "    for j in range(batch_size):\n",
    "        imgs  += [data_batch['inputs'][j][:3].permute(1,2,0).detach().cpu().numpy()+mean]\n",
    "        masks += [data_batch['data_samples']['gt_sem_seg'][j].detach().cpu().numpy()]\n",
    "        preds += [outputs[j].detach().cpu().numpy()]\n",
    "        if len(atn):\n",
    "            aux_feats_0 += [aux_feat[0][j].detach().cpu().numpy()]\n",
    "            aux_feats_1 += [aux_feat[1][j].detach().cpu().numpy()]\n",
    "            atns += [atn[j].detach().cpu().numpy()]\n",
    "    return imgs, masks, preds, aux_feats_0, aux_feats_1, atns\n",
    "       \n",
    "def dec(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "for idx, data_batch in tqdm(enumerate(ValDataloader)):\n",
    "    #with autocast(enabled=False):\n",
    "    with torch.no_grad():\n",
    "        #outputs   = model.val_step(data_batch)\n",
    "        #data_batch             = model.data_preprocessor(data_batch, True)\n",
    "        feats, aux_feat, atn,_ = model.extract_feat(data_batch['inputs'].float().to('cuda'))\n",
    "        outputs = model.decode_head(feats)\n",
    "        if outputs.shape[-1] != img_size:\n",
    "           outputs = FF.resize(outputs,(img_size, img_size), interpolation = FF.InterpolationMode.BILINEAR)\n",
    "        logits = F.softmax(outputs) #model(data_batch['inputs'].float().cuda()), dim=1)\n",
    "        preds  = torch.max(logits, dim=1)[1]\n",
    "        #continue\n",
    "        with open(f'{exp_path}/res/res_{idx}.pkl','wb') as file:\n",
    "            pickle.dump(dec_io(data_batch, preds, aux_feat, atn), file)\n",
    "        \n",
    "    #continue\n",
    "    for j in range(batch_size):\n",
    "        fig, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (35,4))\n",
    "        axes[0].imshow(data_batch['inputs'][j][:3].permute(1,2,0).detach().cpu().numpy()+mean)\n",
    "        axes[1].imshow(data_batch['data_samples']['gt_sem_seg'][j].detach().cpu().numpy(), cmap = cmap, norm = norm)\n",
    "        axes[2].imshow(preds[j].detach().cpu().numpy(), cmap = cmap, norm = norm)\n",
    "        if len(aux_feat):\n",
    "            aux_edge, aux_sal = aux_feat[0][j][0], aux_feat[1][j]\n",
    "            temp     = aux_edge[2:-2,2:-2]\n",
    "            aux_edge = aux_edge * 0.0\n",
    "            aux_edge[2:-2,2:-2] = temp\n",
    "            _max    = aux_sal.max(axis=-1)[0].max(axis=-1)[0][...,None,None]\n",
    "            aux_sal = aux_sal/ _max\n",
    "            axes[3].imshow(aux_edge.detach().cpu().numpy())\n",
    "            axes[4].imshow(aux_sal.max(axis=0)[0].detach().cpu().numpy())\n",
    "        for k, ax in enumerate(axes):\n",
    "            #ax.set_title(f'{titles[k]}')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        fig.savefig(f'{exp_path}/figs/res_{idx*batch_size + j}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb61e1-cef0-4336-8a72-9f057b0ef8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/projects/Deep-Learning-For-Remote-Sensing/libs/mmsegmentation/mmseg/evaluation/metrics/iou_metric.py:197: UserWarning: _histc_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)\n",
      "  area_intersect = torch.histc(\n",
      "/projects/Deep-Learning-For-Remote-Sensing/libs/mmsegmentation/mmseg/evaluation/metrics/iou_metric.py:200: UserWarning: _histc_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)\n",
      "  area_pred_label = torch.histc(\n",
      "/projects/Deep-Learning-For-Remote-Sensing/libs/mmsegmentation/mmseg/evaluation/metrics/iou_metric.py:203: UserWarning: _histc_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:71.)\n",
      "  area_label = torch.histc(\n",
      "109it [00:25,  3.91it/s]"
     ]
    }
   ],
   "source": [
    "val_evaluator.results = []\n",
    "num_classes = 5\n",
    "for idx, data_batch in tqdm(enumerate(ValDataloader)):\n",
    "    with torch.no_grad():\n",
    "        #outputs   = model.val_step(data_batch)\n",
    "        #data_batch             = model.data_preprocessor(data_batch, True)\n",
    "        preds = model.predict(data_batch['inputs'].float().to('cuda'),data_batch['data_samples'])\n",
    "        val_evaluator.process(data_samples=preds, data_batch = None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea5436-970c-4cac-9c3f-42d6ab1be4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "res = val_evaluator.compute_metrics(val_evaluator.results)\n",
    "pd.DataFrame(res, index = ['']).to_csv(f'{exp_path}/best_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163c44c-566e-47e7-8515-67d714864439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_scores(model, ValDataloader, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f4df7-b828-495a-9bfa-f91dc9f83934",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{exp_path}/best_result_all.txt','w') as file:\n",
    "    file.write(val_evaluator.log.get_csv_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e543a-dec3-479e-beed-c9a4bcbff288",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res, index = [''])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c0100be-74f4-492d-a5df-1c2d8d78677d",
   "metadata": {},
   "source": [
    "from mmseg.models import ResNet\n",
    "import torch\n",
    "self = ResNet(depth=18)\n",
    "self.eval()\n",
    "inputs = torch.rand(1, 3, 32, 32)\n",
    "level_outputs = self.forward(inputs)\n",
    "for level_out in level_outputs:\n",
    "    print(tuple(level_out.shape))\n",
    "\n",
    "'''\n",
    " {'type': 'ResNetV1c',\n",
    " 'depth': 50,\n",
    " 'num_stages': 4,\n",
    " 'out_indices': (0, 1, 2, 3),\n",
    " 'dilations': (1, 1, 2, 4),\n",
    " 'strides': (1, 2, 1, 1),\n",
    " 'norm_cfg': {'type': 'SyncBN', 'requires_grad': True},\n",
    " 'norm_eval': False,\n",
    " 'style': 'pytorch',\n",
    " 'contract_dilation': True}\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b76da-4b7a-4918-bc86-318dfbbd94ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
